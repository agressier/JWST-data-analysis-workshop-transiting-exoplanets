{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6867104",
   "metadata": {},
   "source": [
    "# JWST Data Analysis Workshop: “Science Sessions”\n",
    "## The Transiting Exoplanet session\n",
    "\n",
    "This notebook is made for the workshop sessions of the *Spring Symposium 2023 'Planetary systems and the origins of life in the era of JWST'*. We are going to go trough simple steps of JWST data analysis to produce a transit spectrum of **WASP-39 b using ERS NIRSpec PRISM observations**.\n",
    "<br><br>\n",
    "authors: Amélie Gressier (agressier@stsci.edu), Natalie Allen (nallen19@jhu.edu) and Néstor Espinoza (nespinoza@stsci.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6bf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import median_filter\n",
    "from jwst import datamodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder ='/Users/agressier/Documents/jwst_data/0366-nirspec-prism/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e020d6",
   "metadata": {},
   "source": [
    "### Introduction: let's take a look at the data\n",
    "\n",
    "We will work with the `*rampfitstep.fits` files. These are products of the stage 1 JWST detector pipeline where the first steps of the pipeline are performed, i.e., linearity, superbias, saturation correction. \n",
    "<br><br>\n",
    "First, let's load the data and take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eebf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load each segment using datamodels\n",
    "tso1 = datamodels.open(datafolder+'jw01366004001_04101_00001-seg001_nrs1_1_rampfitstep.fits')\n",
    "tso2 = datamodels.open(datafolder+'jw01366004001_04101_00001-seg002_nrs1_1_rampfitstep.fits')\n",
    "tso3 = datamodels.open(datafolder+'jw01366004001_04101_00001-seg003_nrs1_1_rampfitstep.fits')\n",
    "tso4 = datamodels.open(datafolder+'jw01366004001_04101_00001-seg004_nrs1_1_rampfitstep.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a858677",
   "metadata": {},
   "source": [
    "We can use the search function to get metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763752b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tso1.search('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808de78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tso1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ec904",
   "metadata": {},
   "source": [
    "The dimensions of each segment are (`nintegrations, nrows, ncolumns`). The integrations corresponds to the time dimension of the transit/eclipse observation, the number of rows and columns are the dimension of the detector subarray.\n",
    "<br><br>\n",
    "Let's put all the segments into one single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data on a single array:\n",
    "tso = np.vstack((tso1.data, tso2.data))\n",
    "tso = np.vstack((tso, tso3.data))\n",
    "tso = np.vstack((tso, tso4.data))\n",
    "\n",
    "# Errors:\n",
    "tso_err = np.vstack((tso1.err, tso2.err))\n",
    "tso_err = np.vstack((tso_err, tso3.err))\n",
    "tso_err = np.vstack((tso_err, tso4.err))\n",
    "\n",
    "# Data-quality flags:\n",
    "tso_dq = np.vstack((tso1.dq, tso2.dq))\n",
    "tso_dq = np.vstack((tso_dq, tso3.dq))\n",
    "tso_dq = np.vstack((tso_dq, tso4.dq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the times of the observations\n",
    "times = np.append(tso1.int_times['int_mid_BJD_TDB'], tso2.int_times['int_mid_BJD_TDB'])\n",
    "times = np.append(times, tso3.int_times['int_mid_BJD_TDB'])\n",
    "times = np.append(times, tso4.int_times['int_mid_BJD_TDB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918444ce",
   "metadata": {},
   "source": [
    "The transit of WASP-39 b was observed with the NIRSpec PRISM mode and the SUB512 subarray (32x512). The observation is centred arount the transit and lasts 8h23, which corresponds to 21500 integrations in a single exposure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447de727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a median frame\n",
    "median_tso = np.median(tso, axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the median frame\n",
    "plt.figure(figsize=(15,5))\n",
    "im = plt.imshow(median_tso, origin = 'lower', aspect = 'auto')\n",
    "im.set_clim(-2,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c11e",
   "metadata": {},
   "source": [
    "Let's see if we can observe the transit from this first image ! We add the light inside a small portion of the median image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23391a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "postage = tso[:,10:20, 200:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44856873",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = np.sum(postage, axis = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(timeseries / np.median(timeseries[0:50]), '.', color = 'purple',ms=0.5)\n",
    "plt.ylim(0.97,1.006)\n",
    "plt.xlim(0, len(timeseries))\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "    \n",
    "plt.ylabel('Relative flux', fontsize = 18)\n",
    "plt.xlabel('Integration number', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f6266",
   "metadata": {},
   "source": [
    "Amazing ! We have our first transit from JWST data ! We can see a ramp in time, this is a systematic effect that needs to be corrected. We will try to fix it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c49f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some indexes in and out of the transit event that will be useful later.\n",
    "t_integrations = np.arange(len(times))\n",
    "idx_oot = np.where((t_integrations<7000)|(t_integrations>17500))\n",
    "idx_it = np.where((t_integrations>10000)&(t_integrations<14000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45384337",
   "metadata": {},
   "source": [
    "### 1- Stellar flux extraction\n",
    "\n",
    "Let's extract the stellar spectrum in the simpliest way, by adding the flux coming from all the pixels in a given column in the range of y-pixels we select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some range in the x-axis and y-axis to extract the flux\n",
    "xp = np.arange(8,490,1)\n",
    "yp = np.array([15]*len(xp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "im = plt.imshow(median_tso, origin = 'lower', aspect = 'auto')\n",
    "plt.plot(xp,yp, lw=3, color='r')\n",
    "plt.plot(xp,yp-10, lw=3, color='r')\n",
    "plt.plot(xp,yp+10, lw=3, color='r')\n",
    "im.set_clim(-2,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add all the flux in the given y range for each column x and each integration.\n",
    "spectra_extracted = np.zeros([tso.data.shape[0], len(xp)])\n",
    "spectra_extracted_err = np.zeros([tso.data.shape[0], len(xp)])\n",
    "for i in range (0,len (xp)) :\n",
    "    spectra_extracted[:,i] = np.nansum(tso[:,5:25,xp[i]], axis = (1))\n",
    "    spectra_extracted_err[:,i] = np.sqrt(np.nansum((tso_err[:,5:25,xp[i]])**2, axis = (1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36136689",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_extracted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for i in range(spectra_extracted.shape[0]):    \n",
    "    plt.plot(spectra_extracted[i,:], color = 'purple', alpha = 0.1, lw=1)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('Pixel', fontsize = 18)\n",
    "plt.ylabel('DN /s ', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab4956",
   "metadata": {},
   "source": [
    "We can clearly see the saturated part around pixel 100. Let's get rid of some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mad_sigma(x, median):\n",
    "    \"\"\"\n",
    "    This function returns the MAD-based standard-deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    mad = np.nanmedian( np.abs ( x - median ) )\n",
    "\n",
    "    return 1.4826*mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_spectra = np.zeros(spectra_extracted.shape)\n",
    "\n",
    "for i in range(spectra_extracted.shape[0]):\n",
    "    master_spectra[i, :] = spectra_extracted[i,:] / np.nanmedian(spectra_extracted[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_spectrum = np.zeros(spectra_extracted.shape[1])\n",
    "sigma_master_spectrum = np.zeros(spectra_extracted.shape[1])\n",
    "\n",
    "\n",
    "for i in range(spectra_extracted.shape[1]):\n",
    "    median = np.nanmedian(master_spectra[:, i])\n",
    "    master_spectrum[i], sigma_master_spectrum[i] = median, \\\n",
    "                                                     get_mad_sigma(median, master_spectra[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_spectra = np.copy(spectra_extracted)\n",
    "corrected_spectra_err = np.copy(spectra_extracted_err)\n",
    "\n",
    "\n",
    "for i in range(spectra_extracted.shape[0]):\n",
    "    \n",
    "    # Get median to scale:\n",
    "    median = np.median(spectra_extracted[i, :])\n",
    "\n",
    "    # Scale master spectrum and sigma:\n",
    "    model = master_spectrum * median\n",
    "    sigma = sigma_master_spectrum * median\n",
    "    \n",
    "    # Identify bad pixels/columns:\n",
    "    residuals = np.abs(spectra_extracted[i, :] - model)\n",
    "    idx_bad = np.where(residuals > 5 * sigma)[0]\n",
    "    \n",
    "    # Replace:\n",
    "    if len(idx_bad) != 0:\n",
    "        \n",
    "        corrected_spectra[i, idx_bad] = model[idx_bad]\n",
    "        corrected_spectra_err[i, idx_bad] = sigma[idx_bad]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093426c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for i in range(spectra_extracted.shape[0]):    \n",
    "    plt.plot(corrected_spectra[i,:], color = 'purple', alpha = 0.1, lw=1)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('Pixel', fontsize = 18)\n",
    "plt.ylabel('DN /s ', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d0092",
   "metadata": {},
   "source": [
    "This looks great ! First, let's transform the x-pixels into wavelengths. The indexes I used are set to match our extraction to the NIRSpec PRISM wavelength solution. Let's not change those right now but bare in mind that this will have to be modified if the extraction range changes. To get the wavelength solution correclty you can take a look at this tutorial: https://stsci.box.com/s/6c2tg86ruzpxykp8b7pvd1q2ysdnc3p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_solution=np.loadtxt('/Users/agressier/Documents/jwst_workbooks/ERS_WASP-39b_nirspec_wk/wavelength_solutions_prism.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57819252",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = {}\n",
    "spectra['wavelength'] = wavelength_solution\n",
    "spectra['spectra'] = []\n",
    "spectra['errors'] = []\n",
    "\n",
    "for i in range(tso.shape[0]):\n",
    "    spectra['spectra'].append(corrected_spectra[i, 22:454]) # let's not change these indexes for now they match the wavelength solution\n",
    "    spectra['errors'].append(corrected_spectra_err[i, 22:454])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for i in range(spectra_extracted.shape[0]):\n",
    "    plt.plot(spectra['wavelength'],spectra['spectra'][i], color = 'purple', alpha = 0.1, lw=1)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('Wavelength(microns)', fontsize = 18)\n",
    "plt.ylabel('DN /s ', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de9f73",
   "metadata": {},
   "source": [
    "### 2-  White light curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310711f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs['times'] = times + 2400000.5\n",
    "t = lcs['times']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8552f",
   "metadata": {},
   "source": [
    "First, we create the white light curve by integrating over all wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs['white light'] = {}\n",
    "lc = np.array([])\n",
    "lcerr = np.array([])\n",
    "for i in range(len(spectra['spectra'])):\n",
    "    lc = np.append(lc, np.sum(spectra['spectra'][i]))\n",
    "    lcerr = np.append(lcerr, np.sqrt(np.sum(spectra['errors'][i]**2)))\n",
    "\n",
    "lcs['white light']['flux'] = np.copy(lc)\n",
    "lcs['white light']['errors'] = np.copy(lcerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8eaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux, flux_err = lcs['white light']['flux'], lcs['white light']['errors']\n",
    "flux, flux_err = flux / np.nanmedian( flux[idx_oot] ), flux_err / np.nanmedian( flux[idx_oot] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.errorbar(t, flux, flux_err, fmt = '.', color = 'purple', ms = 0.5, elinewidth = 0.5)  \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim(np.min(t), np.max(t))\n",
    "plt.ylim(0.97,1.006)\n",
    "plt.xlabel(r'Time (BJD$_{\\rm TDB}$)', fontsize = 19)\n",
    "plt.ylabel('Relative flux', fontsize = 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc4720",
   "metadata": {},
   "source": [
    "Let's take care of the linear trend we saw before. We fit a line to the out of transit flux and then remove it from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = np.polyfit(t[idx_oot],flux[idx_oot],1)\n",
    "linear_func = np.poly1d(coef) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create an array f_oot for visualization\n",
    "f_int=np.empty((10500))\n",
    "f_int[:]=np.nan\n",
    "f_oot=np.concatenate((flux[:7000],f_int))\n",
    "f_oot=np.concatenate((f_oot,flux[17500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbe884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(r'Time (BJD$_{\\rm TDB}$)', fontsize = 19)\n",
    "plt.ylabel('Relative flux', fontsize = 19)\n",
    "\n",
    "plt.plot(t,f_oot, '.', color='purple', alpha=0.5)\n",
    "plt.plot(t,linear_func(t), '--', color='k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_corrected = flux/linear_func(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(t, flux, flux_err, fmt = '.', color = 'purple', ms = 0.5, elinewidth = 0.5, label ='before linear correction')  \n",
    "plt.errorbar(t, flux_corrected, flux_err, fmt = '.', color = 'mediumseagreen', ms = 0.5, elinewidth = 0.5, alpha=0.5,label ='after linear correction') \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim(np.min(t), np.max(t))\n",
    "plt.xlabel(r'Time (BJD$_{\\rm TDB}$)', fontsize = 19)\n",
    "plt.ylabel('Relative flux', fontsize = 19)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98b385",
   "metadata": {},
   "source": [
    "Congrat's ! We performed our first data analysis correction :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3f3d9",
   "metadata": {},
   "source": [
    "### 3- Spectral light curves\n",
    "\n",
    "Let's create the spectral light curves at pixel resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs['spectral light curves']={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start saving from the smallest wavelength and up:\n",
    "idx_non_zero = np.where( (spectra['wavelength'] != 0.0) & ~(np.isnan(spectra['wavelength']))  )[0]\n",
    "current_wavelength = np.min(spectra['wavelength'][idx_non_zero])\n",
    "\n",
    "while True:\n",
    "    idx = np.where(spectra['wavelength'] == current_wavelength)[0]\n",
    "    lc = np.array([])\n",
    "    lcerr = np.array([])\n",
    "    for i in range(len(spectra['spectra'])):\n",
    "        lc = np.append(lc, spectra['spectra'][i][idx])\n",
    "        lcerr = np.append(lcerr, spectra['errors'][i][idx])\n",
    "\n",
    "    lcs['spectral light curves'][current_wavelength] = {}\n",
    "    lcs['spectral light curves'][current_wavelength]['flux'] = np.copy(lc)\n",
    "    lcs['spectral light curves'][current_wavelength]['errors'] = np.copy(lcerr)\n",
    "\n",
    "    # Go to the next wavelengths:\n",
    "    idx_next = np.where((spectra['wavelength'] != current_wavelength) & (spectra['wavelength'] > current_wavelength))[0]\n",
    "\n",
    "    if len(idx_next) == 0:\n",
    "            break\n",
    "    else:\n",
    "            current_wavelength = np.min(spectra['wavelength'][idx_next])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,9))\n",
    "delta = 0.05\n",
    "\n",
    "counter = 0\n",
    "\n",
    "pixels = spectra['wavelength'][15:25]\n",
    "\n",
    "colormap = plt.get_cmap('cool')\n",
    "\n",
    "color = [colormap(k) for k in np.linspace(0.4, 1.0, len(pixels))]\n",
    "\n",
    "for i,c in zip(pixels, color):\n",
    "    lc, lcerr = lcs['spectral light curves'][i]['flux'], lcs['spectral light curves'][i]['errors']\n",
    "    rel_lc = (lc / np.median(lc[idx_oot]))/linear_func(t)\n",
    "    rel_lc_err = lcerr / np.median(lc[idx_oot])\n",
    "    plt.plot(t, rel_lc + counter * delta, '.',ms=0.5,\n",
    "                 alpha = 0.5, zorder = 1, color = c)\n",
    "    \n",
    "    \n",
    "    plt.text(t[1000],rel_lc[0]+0.01 + counter * delta, \"{:0.4f}\".format(i) + str(r' ${\\mu}$m'), fontsize=11)\n",
    "    counter += 1\n",
    "\n",
    "plt.xlim(np.min(t), np.max(t))\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.xlabel(r'Time (BJD$_{\\rm TDB}$)', fontsize = 14)\n",
    "plt.ylabel('Relative flux + offset', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7dbe6e",
   "metadata": {},
   "source": [
    "### 4- Transmission spectrum\n",
    "\n",
    "Were are going to create a transmission spectrum with a simple computation using the spectral light curves. The correct way of doing it is to fit the spectral light curves with a transit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649822e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute weighted mean and errors\n",
    "def waverage(x, x_err):\n",
    "    x=np.array(x)\n",
    "    xerr=np.array(x_err)\n",
    "    wx=1/(x_err**2)\n",
    "    wav =np.sum(x*wx,0)/np.sum(wx,0)\n",
    "    wav_err= 1/np.sqrt((np.sum(wx,0)))\n",
    "    return wav, wav_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87579c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = list(lcs['spectral light curves'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439e36a",
   "metadata": {},
   "source": [
    "We can do it at the pixel resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_spectrum = np.zeros((len(wavs),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(wavs)):\n",
    "    transmission_spectrum[i,0] = wavs[i]\n",
    "    lc= lcs['spectral light curves'][wavs[i]]['flux']\n",
    "    lcerr= lcs['spectral light curves'][wavs[i]]['errors']\n",
    "    \n",
    "    xin, xerrin = waverage(lc[idx_it],lcerr[idx_it])\n",
    "    xout, xerrout = waverage(lc[idx_oot],lcerr[idx_oot])\n",
    "    transmission_spectrum[i,1]=(xout-xin)/np.median(lc[idx_oot])\n",
    "    transmission_spectrum[i,2] = np.sqrt(xerrin**2+xerrout**2)/np.median(lc[idx_oot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481becba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.errorbar(transmission_spectrum[:,0],transmission_spectrum[:,1], yerr=transmission_spectrum[:,2], fmt='.', color = 'purple', mec='k', elinewidth=1,capsize=1, ms=8)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Wavelength (microns)', fontsize =20)\n",
    "plt.ylabel('Transit depth', fontsize =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227967eb",
   "metadata": {},
   "source": [
    "Let's get rid of some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = transmission_spectrum[:,1] - median_filter(transmission_spectrum[:,1], 11)\n",
    "sigma = get_mad_sigma(np.nanmedian(residuals), residuals)\n",
    "idx_outliers = np.where(np.abs(residuals) > 5*sigma)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(idx_outliers) > 0:\n",
    "            \n",
    "            print('Found', len(idx_outliers), '! Replacing...')\n",
    "            \n",
    "            for idx in idx_outliers:\n",
    "                \n",
    "                if idx == 0:\n",
    "                    \n",
    "                    replacement = 1\n",
    "                    \n",
    "                elif idx == len(residuals)-1:\n",
    "                    \n",
    "                    replacement = len(residuals)-2\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    replacement = idx + 1\n",
    "                    \n",
    "                    if replacement in idx_outliers:\n",
    "                        if replacement == len(residuals) -1:\n",
    "                            replacement = replacement -2\n",
    "                        else:    \n",
    "                        \n",
    "                            replacement = idx + 2\n",
    "                        \n",
    "                            transmission_spectrum[idx, 1] =transmission_spectrum[replacement, 1]\n",
    "                            transmission_spectrum[idx, 2] =transmission_spectrum[replacement, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.errorbar(transmission_spectrum[:,0],transmission_spectrum[:,1], yerr=transmission_spectrum[:,2], fmt='.', color = 'purple', mec='k', elinewidth=1,capsize=1, ms=8)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Wavelength (microns)', fontsize =20)\n",
    "plt.ylabel('Transit depth', fontsize =20)\n",
    "plt.ylim(0.021,0.0245)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2d875",
   "metadata": {},
   "source": [
    "There are still some outliers in particular in the saturated region, but if we zoom in, the transmission spectrum looks pretty nice !\n",
    "<br>\n",
    "You can try at home to create a binned version of this spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c911bc6",
   "metadata": {},
   "source": [
    "You can simply suppress the last outliers for the spectral interpretation with the next lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae513141",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_outliers_full = np.where((transmission_spectrum[:,1]<0.0215)|(transmission_spectrum[:,1]>0.0245) )\n",
    "transmission_spectrum[idx_outliers_full,1] =np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.errorbar(transmission_spectrum[:,0],transmission_spectrum[:,1], yerr=transmission_spectrum[:,2], fmt='.', color = 'purple', mec='k', elinewidth=1,capsize=1, ms=8)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Wavelength (microns)', fontsize =20)\n",
    "plt.ylabel('Transit depth', fontsize =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2960f17",
   "metadata": {},
   "source": [
    "Great ! We have an first approximate of the transmission spectrum of WASP-39 b between 0.5 and 5 microns. Let's save your favorite results and try to interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('w39b_nirspec_prism_pixel_resolution.dat', transmission_spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c972e",
   "metadata": {},
   "source": [
    "### 5-Conclusion and going further\n",
    "\n",
    "We performed our first JWST data analysis using simple tools and functions. This is a **first insight** into transit spectroscopy.\n",
    "To properly extract the stellar spectrum, adjust the light curves and obtain a spectrum, additionnal steps have to be considered. We will not have the time to go into the details of such steps now but you can do it at home.\n",
    "\n",
    "I suggest you ceate a python environment and install some tools. For JWST data analysis you can use _transitspectroscopy_ or the _Eureka_ pipeline. Many tools exist for the light curve fitting, you can install _juliet_ for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import juliet\n",
    "import transitspectroscopy as ts\n",
    "import copy\n",
    "import pickle\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c94f39",
   "metadata": {},
   "source": [
    "**Handling background and 1/f noise**\n",
    "\n",
    "You might want to subtract the background and correct for the 1/f noise before extracting the stellar spectrum. Next is a set of functions to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_1f(median_frame, frame, x, y, min_bkg = 20, max_bkg = 35, mask = None, scale_factor = 1.):\n",
    "    \n",
    "    new_frame = np.copy(frame)\n",
    "    \n",
    "    # The scale-factor accounts for the fact that the transit \"dims\" the frame (which is almost completely \n",
    "    # illuminated). Transit spectral features add only a tiny ammount of scaling (below our precisions), \n",
    "    # so we don't need to account for that:\n",
    "    ms = frame - median_frame * scale_factor\n",
    "    \n",
    "    # Go column-by-column substracting values around the trace:\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        column = x[i]\n",
    "        row = int(y[i])\n",
    "        \n",
    "        min_row = np.max([0, row - 15])\n",
    "        max_row = np.min([32, row + 15])\n",
    "        \n",
    "        bkg = np.append(ms[min_row:row-20, column], ms[row+20:max_row, column])\n",
    "        new_frame[:, column] = new_frame[:, column] - np.nanmedian(bkg)\n",
    "        \n",
    "    return new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "postage = tso[:,10:20, 200:250]\n",
    "timeseries = np.nansum(postage, axis = (1,2))\n",
    "# Create smoothed version:\n",
    "smoothed_petit_transit = median_filter(timeseries / np.median(timeseries[idx_oot]), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp, yp = ts.spectroscopy.trace_spectrum(median_tso, np.median(tso_dq, axis = 0), \n",
    "                                           xstart = 480, ystart = 15, xend = 20 , \n",
    "                                           y_tolerance = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1737d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra1 = np.zeros([tso.data.shape[0], len(xp)])\n",
    "spectra1_err = np.zeros([tso.data.shape[0], len(xp)])\n",
    "\n",
    "for i in range(tso.shape[0]):\n",
    "    \n",
    "    # Note we pass the frames to be corrected by 1/f noise:\n",
    "    bkg_subs_frame = correct_1f(median_tso, \\\n",
    "                                tso[i, :, :] , \\\n",
    "                                xp, yp, \\\n",
    "                                scale_factor = smoothed_petit_transit[i])\n",
    "    \n",
    "    # Then extract the spectrum:\n",
    "    spectra1[i, :], spectra1_err[i, :] = ts.spectroscopy.getSimpleSpectrum(bkg_subs_frame, \n",
    "                                                                                 xp,\n",
    "                                                                                 yp, \n",
    "                                                                                 30, \n",
    "                                                                                 error_data=tso_err[i, :, :], \n",
    "                                                                                 correct_bkg=True,\n",
    "                                                                                 bkg_method ='all')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8435d",
   "metadata": {},
   "source": [
    "**Light curve fitting**\n",
    "\n",
    "We are going to fit the white light curve we extracted before using _juliet_. We use time as a Gaussian Process regressor, and we standardize it for computation efficiency. But first, let's define some planetary and orbital parameter. You can also try to fit with a linear regressors on the linear time corrected or uncorrected flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define planetary parameters:\n",
    "period = 4.0552941\n",
    "# Time-of-transit center:\n",
    "transit_center = 2459787.571354\n",
    "# Update transit center to this day:\n",
    "n = np.ceil((t[0] - transit_center) / period)\n",
    "transit_center = transit_center + n * period\n",
    "# Planet-to-star radius ratio:\n",
    "rprs = 0.1457\n",
    "# Transit duration in days:\n",
    "transit_duration = 2.8032/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa50ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = juliet.utils.get_phases(t, period, transit_center)\n",
    "times_hours = phases * period * 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux, flux_err = lcs['white light']['flux'], lcs['white light']['errors']\n",
    "flux, flux_err = flux / np.nanmedian( flux[idx_oot] ), flux_err / np.nanmedian( flux[idx_oot] )\n",
    "flux_corrected = flux/linear_func(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_regressors(x):\n",
    "    \n",
    "    return (x - np.mean(x)) / np.sqrt( np.var(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = np.zeros([len(t), 1])\n",
    "regressors[:,0] = standarize_regressors(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the parameters to be fit:\n",
    "params = ['P_p1', 't0_p1', 'a_p1', 'b_p1', 'q1_SOSS', 'q2_SOSS', 'ecc_p1', 'omega_p1',\n",
    "          'p_p1', 'mdilution_PRISM', 'mflux_PRISM', 'sigma_w_PRISM','GP_sigma_PRISM', 'GP_rho_PRISM']\n",
    "\n",
    "# Distributions:\n",
    "dists = ['fixed', 'normal', 'normal', 'truncatednormal', 'uniform', 'uniform', 'fixed', 'fixed',\n",
    "         'uniform', 'fixed', 'normal', 'loguniform', 'loguniform', 'loguniform']\n",
    "\n",
    "# Hyperparameters:\n",
    "hyperps = [period, [transit_center,0.2], [11.37, 0.5], [0.447, 0.1, 0., 1.], [0., 1.], [0.,1.], 0., 90.0,\n",
    "           [0., 0.2], 1.0, [0., 0.1], [10., 1000.], [1e-5, 1e3], [1e-3, 0.5]]\n",
    "\n",
    "priors = juliet.generate_priors(params, dists, hyperps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2fdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "times, fluxes, fluxes_error = {}, {}, {}\n",
    "    \n",
    "times['PRISM'], fluxes['PRISM'], fluxes_error['PRISM'] = t,flux_corrected,flux_err\n",
    "\n",
    "reg = {}\n",
    "reg['PRISM'] = regressors\n",
    "\n",
    "# Perform juliet fits:\n",
    "dataset = juliet.load(priors=priors, t_lc=times, y_lc=fluxes, \\\n",
    "                                               yerr_lc=fluxes_error, GP_regressors_lc=reg,\\\n",
    "                                               out_folder='W39b_wlc_fitting_GP_corr', \\\n",
    "                                               ld_laws = 'squareroot')\n",
    "        \n",
    "results = dataset.fit(sampler = 'dynamic_dynesty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_center = np.median(results.posteriors['posterior_samples']['t0_p1'])\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "normalized_time = (t - transit_center) * 24\n",
    "sigma_PRISM = np.median(results.posteriors['posterior_samples']['sigma_w_PRISM']*1e-6)\n",
    "total_errors = np.sqrt(sigma_PRISM**2 + dataset.errors_lc['PRISM']**2)\n",
    "    \n",
    "plt.errorbar(normalized_time, \\\n",
    "                 dataset.data_lc['PRISM'], \\\n",
    "                 yerr = total_errors, \\\n",
    "                 fmt = '.', color = 'purple', ms = 4, elinewidth = 1, zorder = 1)\n",
    "    \n",
    "# Evaluate model:\n",
    "transit_model, transit_up68, transit_low68  = \\\n",
    "                   results.lc.evaluate('PRISM', return_err=True)\n",
    "    \n",
    "transit_model, transit_up99, transit_low99  = \\\n",
    "                   results.lc.evaluate('PRISM', return_err=True, alpha = 0.99)\n",
    "        \n",
    "plt.plot(normalized_time, transit_model, color = 'black', lw=3, zorder=2)\n",
    "        \n",
    "plt.fill_between(normalized_time, transit_up68, transit_low68, \\\n",
    "                     color='purple',alpha=0.5, zorder=3)\n",
    "    \n",
    "plt.fill_between(normalized_time, transit_up99, transit_low99, \\\n",
    "                     color='purple',alpha=0.5, zorder=3)\n",
    "    \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim(np.min(normalized_time), np.max(normalized_time))\n",
    "plt.title('White-lightcurve', fontsize = 18)\n",
    "plt.xlabel('Time from mid-transit (hours)', fontsize = 14)\n",
    "plt.ylabel('Relative flux', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f19049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "# Plot corner plot:\n",
    "stacked_samples = np.vstack((np.vstack((results.posteriors['posterior_samples']['a_p1'], \\\n",
    "                                        results.posteriors['posterior_samples']['b_p1'])), \\\n",
    "                                       (results.posteriors['posterior_samples']['p_p1']**2)*1e6)).T\n",
    "\n",
    "figure = corner.corner(stacked_samples, labels = [r\"$a/R_*$\", r\"$b$ ($a/R_* \\cos(i)$)\", r\"Depth (ppm)\"],\\\n",
    "                       quantiles=[0.16, 0.5, 0.84],\\\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 10})\n",
    "\n",
    "true_values = [11.37, 0.447, (0.14576**2)*1e6] # best guesses from the NASA Exoplanet archive\n",
    "axes = np.array(figure.axes).reshape((3, 3)) \n",
    "\n",
    "# Loop over histograms:\n",
    "for i in range(3):\n",
    "    ax = axes[i,i]\n",
    "    ax.axvline(true_values[i], color = 'purple')\n",
    "\n",
    "# Now loop over 2D surfaces:\n",
    "for yi in range(3):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi] \n",
    "        ax.axvline(true_values[xi], color = 'purple')\n",
    "        ax.axhline(true_values[yi], color = 'purple')\n",
    "        ax.plot(true_values[xi], true_values[yi], 'o', mfc = 'purple', mec = 'purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34d8f9",
   "metadata": {},
   "source": [
    "You can then use the same process to fit the spectral light curve and obtain a transmission spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 2459787.5567417163\n",
    "a = 11.328210411169186\n",
    "b = 0.45860700663403464\n",
    "\n",
    "\n",
    "#Saving data for spectral light curves fitting\n",
    "\n",
    "# First, define common priors for all the datasets\n",
    "\n",
    "params = ['P_p1', 't0_p1', 'a_p1', 'b_p1', 'q1_SOSS', 'q2_SOSS', 'ecc_p1', 'omega_p1', \n",
    "          'p_p1', 'mdilution_PRISM', 'mflux_PRISM', 'sigma_w_PRISM', 'GP_sigma_PRISM', 'GP_rho_PRISM']\n",
    "\n",
    "dists = ['fixed', 'fixed', 'fixed', 'fixed', 'uniform', 'uniform', 'fixed', 'fixed', \n",
    "         'uniform', 'fixed', 'normal', 'loguniform', 'loguniform', 'loguniform']\n",
    "\n",
    "hyperps = [period, t0, a, b, [ 0., 1.], [0., 1.], 0., 90., \n",
    "          [0., 0.2], 1.0, [0., 0.1], [10., 10000.], [1e-6, 1e2], [1e-5, 1e3]]\n",
    "    \n",
    "    \n",
    "prior = juliet.generate_priors(params, dists, hyperps)\n",
    "\n",
    "\n",
    "# Same for starting points:\n",
    "starting_point = {}\n",
    "starting_point['p_p1'] = 0.1\n",
    "starting_point['mflux_PRISM'] = 0.0\n",
    "starting_point['sigma_w_PRISM'] = 100.\n",
    "starting_point['GP_sigma_PRISM'] = 0.1\n",
    "starting_point['GP_rho_PRISM'] = 0.1\n",
    "starting_point['q1_PRISM'] = 0.\n",
    "starting_point['q2_PRISM'] = 0.\n",
    "\n",
    "\n",
    "# Now paste datasets:\n",
    "\n",
    "data_dictionary = {}\n",
    "priors = {}\n",
    "starting_points = {}\n",
    "wavelengths = list(lcs_binned['spectral light curves'].keys())\n",
    "idx = np.where(~np.isnan(wavelengths))\n",
    "wavelengths = np.array(wavelengths)[idx]\n",
    "idx = np.argsort(wavelengths)\n",
    "wavelengths = wavelengths[idx] \n",
    "for i in range(0, len(wavelengths)):\n",
    "        \n",
    "        mean_wavelength = wavelengths[i]\n",
    "        name = 'wbin_' + str( np.round(mean_wavelength, 4) )\n",
    "        \n",
    "        # Create dictionary for current wavelength bin:\n",
    "        data_dictionary[name] = {}\n",
    "        \n",
    "        # Fill data in:\n",
    "        data_dictionary[name]['times'] = t\n",
    "        data_dictionary[name]['GP_external_parameters'] = t\n",
    "        \n",
    "        data_dictionary[name]['flux'] = 0.\n",
    "        data_dictionary[name]['error'] = 0.\n",
    "        \n",
    "        data_dictionary[name]['flux'] =  lcs_binned['spectral light curves'][mean_wavelength]['flux'] \n",
    "        data_dictionary[name]['error'] = lcs_binned['spectral light curves'][mean_wavelength]['errors']**2\n",
    "            \n",
    "        oot_flux = np.nanmedian(data_dictionary[name]['flux'][:500])\n",
    "            \n",
    "        data_dictionary[name]['flux'] = (data_dictionary[name]['flux'] / oot_flux)\n",
    "        data_dictionary[name]['error'] = np.sqrt(data_dictionary[name]['error']) / oot_flux\n",
    "        \n",
    "        # Replace extreme outliers:\n",
    "        residuals = data_dictionary[name]['flux'] - median_filter(data_dictionary[name]['flux'], 11)\n",
    "        sigma = ts.utils.get_mad_sigma(np.nanmedian(residuals), residuals)\n",
    "        idx_outliers = np.where(np.abs(residuals) > 5*sigma)[0]\n",
    "        \n",
    "        if len(idx_outliers) > 0:\n",
    "            \n",
    "            print('Found', len(idx_outliers), 'outliers on', name,'! Replacing...')\n",
    "            \n",
    "            for idx in idx_outliers:\n",
    "                \n",
    "                if idx == 0:\n",
    "                    \n",
    "                    replacement = 1\n",
    "                    \n",
    "                elif idx == len(residuals)-1:\n",
    "                    \n",
    "                    replacement = len(residuals)-2\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    replacement = idx + 1\n",
    "                    \n",
    "                    if replacement in idx_outliers:\n",
    "                        if replacement == len(residuals) -1:\n",
    "                            replacement = replacement -2\n",
    "                        else:    \n",
    "                        \n",
    "                            replacement = idx + 2\n",
    "                        \n",
    "                            data_dictionary[name]['flux'][idx] = data_dictionary[name]['flux'][replacement]\n",
    "                            data_dictionary[name]['error'][idx] = data_dictionary[name]['error'][replacement]\n",
    "        \n",
    "        # Generate priors and starting_points:\n",
    "        priors[name] = copy.deepcopy(prior)\n",
    "        starting_points[name] = copy.deepcopy(starting_point)\n",
    "        \n",
    "print( 'lcs', len(data_dictionary.keys()))\n",
    "# Dump pickle files:\n",
    "pickle.dump(data_dictionary, open('unbinned_data_dictionary_w39b_bin.pkl', 'wb'))\n",
    "pickle.dump(priors, open('unbinned_priors_w39b_bin.pkl', 'wb'))\n",
    "pickle.dump(starting_points, open('unbinned_starting_points_w39b_bin.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jwstt]",
   "language": "python",
   "name": "conda-env-jwstt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
